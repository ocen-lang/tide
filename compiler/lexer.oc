//* The lexer

import std::vector::Vector
import std::span::{Span, Location}
import std::buffer::Buffer
import std::sv::SV
import @errors::Error
import @tokens::{Token, TokenType}

struct Lexer {
    source: str
    source_len: u32
    i: u32
    loc: Location
    seen_newline: bool

    errors: &Vector<&Error>

    in_comment: bool
    comment: Buffer
    comment_start: Location
}

def Lexer::make(source: str, filename: str): Lexer {
    let start_loc = Location(filename, 1, 1, 0)
    return Lexer(
        source,
        source_len: source.len(),
        i: 0,
        loc: start_loc,
        seen_newline: false,
        errors: Vector<&Error>::new(),
        in_comment: false,
        comment: Buffer::make(),
        comment_start: start_loc,
    )
}

def Lexer::finalize(&this, token: Token): Token {
    token.seen_newline = .seen_newline
    if .comment.size > 0 {
        token.comment = .comment.copy().sv()
        token.comment_loc = .comment_start
    }
    .comment.clear()
    .seen_newline = false
    .in_comment = false
    return token
}

def Lexer::simple(&this, type: TokenType, len: u32): Token {
    let start_loc = .loc
    for let i = 0; i < len; i += 1 {
        .inc()
    }
    return .finalize(Token::from_type(type, Span(start_loc, .loc)))
}

def Lexer::cur(&this): char => .source[.i]

def Lexer::inc(&this) {
    match .cur() {
        '\n' => {
            .loc.line += 1
            .loc.col = 1
            .seen_newline = true
        }
        else => .loc.col += 1
    }
    .i += 1
    .loc.index += 1
}

def Lexer::peek(&this, offset: u32 = 1): char {
    if .cur() == '\0' {
        return .cur()
    }
    return .source[.i + offset]
}

def Lexer::lex_char_literal(&this): Token {
    let start_loc = .loc
    let start = .i + 1
    .inc()

    if .cur() == '\\' {
        .inc()
    }
    .inc()
    if .cur() != '\'' {
        .errors.push(Error::new(Span(.loc, .loc), "Expected ' after character literal"))
    }

    let len = .i - start
    let text = SV(.source + start, len)

    .inc()
    return .finalize(Token::new(TokenType::CharLiteral, Span(start_loc, .loc), text))
}

// Format strings can be specified JS-style with backticks, or Python-style with f"..."
// Backticks can be inferred in here directly, but for `f"..."` we need to the lexer to tell
// us whether we saw an `f` right before the string literal or not.
def Lexer::lex_string_literal(&this, has_seen_f: bool): Token {
    let start_loc = .loc
    let end_char = .cur()
    let start = .i + 1
    .inc()
    while .i < .source_len and .cur() != end_char {
        if .cur() == '\\' {
            .inc()
        }
        .inc()
    }

    let len = .i - start
    let text = SV(.source + start, len)
    .inc()

    if .i >= .source_len {
        .errors.push(Error::new(Span(.loc, .loc), "Unterminated string literal"))
    }

    if end_char == '`' or has_seen_f {
        return .finalize(Token::new(TokenType::FormatStringLiteral, Span(start_loc, .loc), text))
    } else {
        return .finalize(Token::new(TokenType::StringLiteral, Span(start_loc, .loc), text))
    }
}

def Lexer::lex_int_literal_different_base(&this): Token {
    let start_loc = .loc
    let start = .i
    .inc()
    match .cur() {
        'x' => {
            .inc()
            while .i < .source_len and .cur().is_hex_digit() {
                .inc()
            }
        }
        'b' => {
            .inc()
            while .i < .source_len and .cur() == '0' or .cur() == '1' {
                .inc()
            }
        }
        else => assert false, "Invalid base for int literal"
    }
    let len = .i - start
    let text = SV(.source + start, len)
    return Token::new(TokenType::IntLiteral, Span(start_loc, .loc), text)
}

def Lexer::lex_numeric_literal_helper(&this): Token {
    let start_loc = .loc
    if .cur() == '0' {
        match .peek(1) {
            'x' | 'b' => {
                return .lex_int_literal_different_base()
            }
            // Do nothing, fall through
            else => {}
        }
    }

    let start = .i
    let token_type: TokenType
    while .cur().is_digit() {
        .inc()
    }
    if .cur() == '.' {
        .inc()
        while .cur().is_digit() {
            .inc()
        }
        token_type = TokenType::FloatLiteral
    } else {
        token_type = TokenType::IntLiteral
    }
    let len = .i - start
    let text = SV(.source+start, len)
    return Token::new(token_type, Span(start_loc, .loc), text)
}

def Lexer::lex_numeric_literal(&this): Token {
    let token = .lex_numeric_literal_helper()
    return .finalize(token)
}

def Lexer::lex_comment(&this) {
    // Skip leading slashes
    while .cur() == '/' { .inc() }

    // We only save comments that have a leading asterisk, dot or exclamation mark
    let save_comment = false
    if .cur() == '*' or .cur() == '.' or .cur() == '!' {
        .inc()
        save_comment = true
        if .comment.size == 0 {
            .comment_start = .loc
        }
    }

    // Skip whitespace
    if .cur() == ' ' or .cur() == '\t' { .inc() }

    // Read the comment and store it into the buffer
    while .i < .source_len and .cur() != '\n' {
        if save_comment then .comment += .cur()
        .inc()
    }

    if save_comment then .comment += '\n'
}

def Lexer::next(&this): Token {
    while .i < .source_len {
        let c = .cur()
        match c {
            ' ' | '\t' | '\v' | '\r' | '\b'| '\n' => {
                .inc()
            }
            ';' => return .simple(TokenType::Semicolon, len: 1)
            ',' => return .simple(TokenType::Comma, len: 1)
            '(' => return .simple(TokenType::OpenParen, len: 1)
            ')' => return .simple(TokenType::CloseParen, len: 1)
            '[' => return .simple(TokenType::OpenSquare, len: 1)
            ']' => return .simple(TokenType::CloseSquare, len: 1)
            '{' => return .simple(TokenType::OpenCurly, len: 1)
            '}' => return .simple(TokenType::CloseCurly, len: 1)
            '@' => return .simple(TokenType::AtSign, len: 1)
            '%' => return .simple(TokenType::Percent, len: 1)
            '^' => return .simple(TokenType::Caret, len: 1)
            '&' => return .simple(TokenType::Ampersand, len: 1)
            '|' => return .simple(TokenType::Line, len: 1)
            '?' => return .simple(TokenType::Question, len: 1)
            '~' => return .simple(TokenType::Tilde, len: 1)
            '.' => {
                if .peek(1) == '.' and .peek(2) == '.' {
                    return .simple(TokenType::Ellipsis, len: 3)
                } else {
                    return .simple(TokenType::Dot, len: 1)
                }
            }
            '!' => return match .peek(1) {
                '='  => .simple(TokenType::NotEquals, len: 2)
                else => .simple(TokenType::Exclamation, len: 1)
            }
            ':' => return match .peek(1) {
                ':'  => .simple(TokenType::ColonColon, len: 2)
                else => .simple(TokenType::Colon, len: 1)
            }
            '=' => return match .peek(1) {
                '='  => .simple(TokenType::EqualEquals, len: 2)
                '>'  => .simple(TokenType::FatArrow, len: 2)
                else => .simple(TokenType::Equals, len: 1)
            }
            '*' => return match .peek(1) {
                '='  => .simple(TokenType::StarEquals, len: 2)
                else => .simple(TokenType::Star, len: 1)
            }
            '+' =>return match .peek(1) {
                '='  => .simple(TokenType::PlusEquals, len: 2)
                '+' => .simple(TokenType::PlusPlus, len: 2)
                else => .simple(TokenType::Plus, len: 1)
            }
            '-' => return match .peek(1) {
                '='  => .simple(TokenType::MinusEquals, len: 2)
                '-' => .simple(TokenType::MinusMinus, len: 2)
                else => .simple(TokenType::Minus, len: 1)
            }
            '<' => return match .peek(1) {
                '='  => .simple(TokenType::LessThanEquals, len: 2)
                else => .simple(TokenType::LessThan, len: 1)
            }
            '>' => return match .peek(1) {
                '='  => .simple(TokenType::GreaterThanEquals, len: 2)
                else => .simple(TokenType::GreaterThan, len: 1)
            }
            '/' => match .peek(1) {
                '/' => .lex_comment()
                '='  => return .simple(TokenType::SlashEquals, len: 2)
                else => return .simple(TokenType::Slash, len: 1)
            }
            '\'' => return .lex_char_literal()
            '"' | '`' => return .lex_string_literal(has_seen_f: false)
            else => {
                let start_loc = .loc

                if c == 'f' and .peek(1) == '"' {
                    .inc()
                    return .lex_string_literal(has_seen_f: true)

                } else if c.is_digit() {
                    return .lex_numeric_literal()

                } else if c.is_alpha() or c == '_' {
                    let start = .i
                    while .cur().is_alnum() or .cur() == '_' {
                        .inc()
                    }
                    let len = .i - start
                    let text = SV(.source + start, len)

                    return .finalize(Token::from_ident(text, Span(start_loc, .loc)))

                } else {
                    .errors.push(Error::new(Span(.loc, .loc), `Unrecognized char in lexer: '{c}'`))
                    .inc()
                }
            }
        }
    }

    // We can assume EOF acts like a newline
    .seen_newline = true
    return .simple(TokenType::EOF, len: 0)
}
